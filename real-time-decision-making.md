# Research direction 1: (near) real-time decision-making

High performance to synchronize with the real-world systems, predict and act in real-time is the key enablers for DTs to deliver holistic value (\citealp{Lacroix2021how}). Current development of mobile communication technology alone is not always enough to enable the holistic understanding. DTs need analytics to help enterprises achieve real-time insights and make real-time data actionable in the moment (\citealp{Daugherty2021Technology}). A discussion about the opportunities and challenges in Industry 4.0 pointed out that solving extremely large problems almost instantaneously offer ample research opportunities in data analytics and optimization methods (\citealp{Olsen2020Industry}). In this subsection, we express our thoughts on the novel decision-making frameworks and efficient algorithms to address the difficulties of the computational complexity and short computation time constraints in the DT context.

The simulation community has proposed useful framework, symbiotic simulation (SS), for real-time decision-making. The SS represents a paradigm in which a physical system and computational model representations are closely associated with each other (\citealp{onggo2021combining}). Unlike the DT, which is intended to span the entire lifecycle of a product, including design, production, operations and disposal (\citealp{Minerva2020digital,Grieves2017Digital}), the SS system is ``designed to support decision making at the \textit{operational} levels by making use of real- or near real-time data (generated by the physical system), and which is streamed subsequent to the development of the simulation model'', according to \cite{onggo2018symbiotic}. The SS system presents a clear integrative framework to support short-term operational management decisions (\citealp{onggo2018symbiotic,Olcott2020digital}). The integrative framework combines components, including data acquisition module, data analytics model, scenario manager, optimization model, simulation model, and machine learning model (\citealp{onggo2018symbiotic}). In fact, since a more resent SS study emphasizes on the ``computational model representation'' (\citealp{onggo2021combining}) rather than a simply ``simulation system'' (\citealp{aydt2008symbiotic,aydt2009research}), it is interesting to see that the understandings of SS and DT gradually converge. We are eager to see that DT and SS research benefits each other. For instance, the DT community can be benefited from the integrative frameworks of simulation, data analytics and machine learning suggested by the SS research (\citealp{onggo2018symbiotic}).

As mentioned by \cite{onggo2021combining}, the SS, as well as the DT, still faces challenges in developing advanced methodologies and suitable algorithms (e.g. optimization, machine learning) in a short-term decision-making environment. The real-time decision-making is related to a long line of work on online problems. According to \cite{patrick2010online}, an online problem is one where the problem data are revealed incrementally. The quality of online algorithms are mainly evaluated by comparing the performance of a strategy with the performance of an optimal strategy that is derived offline with complete information. See some recent theoretical works on dynamic programming (\citealp{vera2021the}), linear optimization (\citealp{garber2021efficient}), and aggregation problem (\citealp{bienkowski2020online}).

In addition to the challenges of incomplete knowledge of the future, real-time problems have to overcome the tight computation time constraints. Exact algorithms always face the complexity of computing the optimal policy due to ``curse of dimensionality'' as well as analytically evaluating the system performance measures due to the complexity and stochastic nature of the system (\citealp{vera2021the,nelson2010optimization}). Classical ways to solve problems at a large scale include the use of heuristics and myopic policies. Unfortunately, the heuristics are usually dedicated to special problems (\citealp{bertsimas2019online}) and myopic policy is globally optimal only in some special cases (for example, see \cite{baucells2019the} for a discussion about the decision situations and utility functions under which myopic strategy is globally optimal).

The importance of addressing (near) real-time problems in complex systems has led to a substantial research effort in the simulation optimization community particularly. For example, to tackle the challenge of high dimensionality, \cite{Xu2013AHA} propose an adaptive hyperbox algorithm whose efficiency is less affected by the problem's dimension increase; \cite{Lu2019Faster} focus on applying kriging to high-dimensional simulators and the proposed method can handle inputs exceeding 10,000 dimensions. For the state of the art in discrete optimization, we refer the interested readers to \cite{semelhago2021rapid}. Such general research efforts in improving optimization capability high-dimensional systems are definitely helpful in short-term decision-making in DT. In particular, we would like to discuss four promising avenues to pursue in the DT context. 


The first is to fully utilize the offline models in real-time problems. The DTs, guided by the domain knowledge, are able to simulate EoP's behavior as exact as possible and offer sufficient detailed information \cite{liu2021review}. The shortcoming of the high-fidelity simulation is the expensive computational time. More and more researchers study how to reuse the simulation efforts to solve real-time problems (see \cite{nelson2016some,Jiang2016a,Ouyang2017simulation}). \cite{Hong2019OSOA} summarize the above works to a unified framework called ``offline simulation online application'' (OSOA) to make real-time decisions in large-scale systems. OSOA treats a simulator as a data generator to train predictive models offline, and it directly uses predictive models for real-time decisions. The authors illustrate how to apply OSOA in three typical problems: estimation, ranking and selection, and simulation optimization. 
\cite{Jiang2020online} apply OSOA to online portfolio risk monitoring by building a logistic regression model, based on data generated in offline simulation experiments. A similar idea called ``plan online learn offline'' (POLO) is proposed by \cite{Kendall2018POLO}, based on Markov decision process (MDP) models. This proposed framework builds on the synergistic relationship between local trajectory optimization, global value function learning, and exploration of uncertain reward. In a real-time vehicle routing problem, \cite{Ulmer2019Offline} combine offline value function approximation with online rollout algorithms to obtain a high-quality and computationally tractable policy for real-time decision-making.

The second is the multi-fidelity approaches. Employing multiple simulation models with varying fidelities has been an area of interest in OR. \cite{Peherstorfer2018Survey} classify multi-fidelity methods into three classes: adapting a low-fidelity model with information from a high-fidelity model, fusing low- and high-fidelity model outputs, and using a high-fidelity model selectively based on information from a low-fidelity model. \cite{Fu2017History} also suggest that the future work in simulation optimization should use a a collection of models in which simple models are used for global search and more granular models are built for local serach. For example, \cite{rhodes2018multi} use multi-fidelity modeling for the the aircraft recovery problem. The proposed method balances the need of using high-fidelity simulations for good estimates with the computational difficulties of a large and complicated solution space and short computation time constraints. Though high-fidelity simulators are emphasized, DTs have the ability to provide different abstractions levels for specific capabilities and services (\citealp{Minerva2020digital}), which makes DT suitable for application of multi-fidelity approaches. To further improve the efficiency, \cite{zhou2021analytics} integrate a state-of-the-art optimal computing budget allocation method (\citealp{chen2011stochastic}) in DTs to determine trade-offs between making enough runs for accurate estimates and computation time. Besides, an important challenge in multi-fidelity approaches is to ``move beyond methods that focus exclusively on models, so that decision-makers can draw on a broader range of available information sources'', as suggested by \cite{Peherstorfer2018Survey}. The DT system might contribute to this challenge due to its ability in integrating historical data, real-time data, and domain knowledge.

The third one relates to the parallel computing. A fundamental function of simulation is to simulate and evaluate different policies or system designs. Generally, a statistical guarantee can be obtained when a potential solution set is small in size. In a digital twin, the number of potential solutions is often too large to simulate all alternatives. A promising approach for large-scale simulation optimization is to leverage the power of parallel computing environments. \cite{Luo2015Fully} modify the traditional fully sequential procedures for multi-core personal computers and many-core servers. They propose two types of fully sequential procedures: vector-filling procedures and asymptotic parallel selection procedures. However, they also report that the directly implementing sequential procedures in parallel computing environments leads to statistical issues. To overcome the inefficiency of directly applying traditional approaches, \cite{Zhong2020Knockout} propose new ``knockout-tournament'' procedures in parallel computing environments. Since required sample sizes increase linearly with the number of alternatives, and the number of communications between processors is minimal, the proposed procedures are well suited for large-scale ranking and selection problems. Future research can further implement parallel computing environments---from ranking and selection to other types of simulation optimization problems. \cite{zhong2021speeding} consider how to speed up a fully sequential procedures, namely, Paulson’s procedure, by reducing the burden of frequent communications and coordination in parallel computing environments. The DT in large systems could lead to highly distributed systems (\citealp{Minerva2020digital}). For example, a high-performance parallel computing approach has been implemented on the top of the virtual models for online analysis in the power grid DT system (\citealp{zhou2019digital}). We see great potential for exploring and applying simulation optimization in parallel computing environments in the DT context. 

## The last one is metamodeling.

In addition, the DT provides several open questions as well.
- The application scope of DTs is enlarged. The interaction between DTs further complicate the problem. Simulators are build to model real-world entities' behavior, reasoning, and their interaction. 
- The usage of a DT for a large system is typically the ``prediction and simulation of the behavior of \textbf{an aggregated set of DTs} in order to understand, control, govern, and orchestrate the behavior of a complex system''\cite{Minerva2020digital}. First, ``the heterogeneous and interconnected components in a complex system''\cite{Lacroix2021how} requires the property of \textbf{composability}, that is ``the ability of grouping several objects into a composed one and then to observe and control the behavior of the composed object as well as the individual components''\cite{Minerva2020digital}.
- \cite{dovgan2019a}. A particular challenge would be the deployment of the algorithm in a real-life vehicle and its evaluation on a real route, where real-life neighboring vehicles and unexpected events have to be considered.
- It is challenging to process the massive and low information density data in real time.
- \textit{Real-time response (interventional)} Moreover, by interacting with \textit{real-time data}, \textit{simulators} can lead to more effective feedback. \cite{Uhlemann2017The} provide an intuitive illustration of real-time production data's benefits in a digital twin. Since this work takes place at a pilot factory, its amount of data remains limited. In a real data-intensive system, conventional decision-making models insufficiently leverage real-time data, and the greatest real-time response challenge is ``managing and analyzing the sheer volume of streaming data coming in from thousands of sources to make sense of it all in real-time'', as \cite{William2020Digital} noted. 




